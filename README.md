# NaturalLanguageProcessing

## Homework 1: Essay Classification with an n-gram Language Model
* part 1: extracting n-grams from sentences
* part 2: counting n-grams in a corpus
* part 3: computing raw n-gram probabilities
* part 4: smoothed probabilities
* part 5: computing senttence probability
* part 6: calculating perplexity
* part 7: using model for text classification

## Homework 2: Parsing with Context Free Grammars
* part 1: reading the grammar
* part 2: membership checking with CKY
* part 3: parsing with backpointers
* part 4: retrieving a parse tree
* part 5: evaluating the parser

## Homework 3: Dependency Parsing with Neural Networks
* part 1: obtaining the vocabularies from one-hot representations and POS tags
* part 2: extracting input/output matrices for training
* part 3: designing and training the network using Keras
* part 4: greedy parsing algorithm â€“ building and evaluating the parser

## Homework 4: Lexical Substitution
* part 1: candidate synonyms from WordNet
* part 2: WordNet frequency baseline
* part 3: simple lesk algorithm
* part 4: most similar synonym
* part 5: using BERT's masked language model
