{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dc3347_homework4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X20GOZmb5F7t",
        "outputId": "e20d34ca-2d62-496f-80a7-66f7ec4b85ce"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxfF6CPC5Ho3",
        "outputId": "82be60b5-6897-46bc-cf8b-a9ab235bf42f"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "wn.lemmas('break', pos='n') # Retrieve all lexemes for the noun 'break'\n",
        "l1 = wn.lemmas('break', pos='n')[0]\n",
        "s1 = l1.synset() # get the synset for the first lexeme\n",
        "s1\n",
        "#s1.lemmas()\n",
        "#s1.lemmas()[0].name() # Get the word of the first lexeme\n",
        "#s1.definition()\n",
        "#s1.examples()\n",
        "#s1.hypernyms()\n",
        "#s1.hyponyms()\n",
        "#l1.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('interruption.n.02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_DTVf3P3QgF",
        "outputId": "57372ab8-5a1a-4bd6-ef6e-a2d8e0bf9df8"
      },
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-12 01:15:05--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.224.99\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.224.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz.1’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  88.9MB/s    in 17s     \n",
            "\n",
            "2021-06-12 01:15:22 (93.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz.1’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLrZ16M-5uJu"
      },
      "source": [
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aN4CV6D8HDM",
        "outputId": "2e7d8c55-0b23-4125-f674-836cb3f9a3a2"
      },
      "source": [
        "v1 = model.wv['computer']\n",
        "#v1 = model.get_vector('computer)\n",
        "v1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
              "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
              "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
              "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
              "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
              "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
              "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
              "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
              "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
              "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
              "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
              "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
              "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
              "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
              "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
              "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
              "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
              "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
              "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
              "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
              "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
              "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
              "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
              "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
              "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
              "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
              "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
              "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
              "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
              "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
              "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
              "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
              "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
              "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
              "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
              "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
              "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
              "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
              "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
              "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
              "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
              "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
              "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
              "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
              "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
              "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
              "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
              "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
              "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
              "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
              "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
              "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
              "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
              "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
              "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
              "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
              "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
              "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
              "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
              "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
              "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
              "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
              "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
              "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
              "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
              "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
              "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
              "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
              "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
              "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
              "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
              "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
              "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
              "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
              "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy7ywJ9y8KCb",
        "outputId": "1951230a-2b9e-4e18-bbd4-44d340135f50"
      },
      "source": [
        "len(v1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrO7vFqP8LYd",
        "outputId": "a2204a73-6235-4361-c7a2-c51739c1714a"
      },
      "source": [
        "model.similarity('computer','calculator')\n",
        "#model.similarity('computer','toaster')\n",
        "#model.similarity('computer','dog')\n",
        "#model.similarity('computer','run')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3339888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gXTXp7O8kRV",
        "outputId": "45f8b0ed-5844-4626-906d-2b5e0e4be66c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeNwCmYqBZeF"
      },
      "source": [
        "#!python lexsub_xml.py lexsub_trial.xml\n",
        "#!head smurf.predict # print the first 10 lines of the file\n",
        "#!perl score.pl smurf.predict gold.trial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWXyGIIjBNr-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "de209e8a-2215-4eb8-a7ac-7287b6586184"
      },
      "source": [
        "# lexsub_main.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "from lexsub_xml import read_lexsub_xml\n",
        "from lexsub_xml import Context \n",
        "\n",
        "# suggested imports \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import gensim\n",
        "import transformers \n",
        "\n",
        "import string\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def tokenize(s): \n",
        "    \"\"\"\n",
        "    a naive tokenizer that splits on punctuation and whitespaces.  \n",
        "    \"\"\"\n",
        "    s = \"\".join(\" \" if x in string.punctuation else x for x in s.lower())    \n",
        "    return s.split() \n",
        "\n",
        "def get_candidates(lemma, pos) -> List[str]:\n",
        "    # Part 1\n",
        "    # pos is 'a','n','v','r' for adj, noun, verb, adv\n",
        "    # Takes a lemma and POS and returns set of possible substitutes\n",
        "\n",
        "    substitutes = []\n",
        "    lemmas = wn.lemmas(lemma, pos)\n",
        "\n",
        "    # Look up the lemma and POS in WordNet and get all synsets that the lemma appears in\n",
        "    for lem in lemmas:\n",
        "      synset = lem.synset()\n",
        "\n",
        "      # Then, obtain all lemmas that appear in any of these synsets\n",
        "      for candidate in synset.lemmas():\n",
        "        candidate_name = candidate.name()\n",
        "        # Make sure output doesn't contain the input lemma itself\n",
        "        if candidate_name == lemma:\n",
        "          continue\n",
        "        else:\n",
        "          # Remove the '_' in 'turn_around' for ex and add to list\n",
        "          substitutes.append(candidate_name.replace(\"_\",\"\"))\n",
        "      \n",
        "    # Return set of possible substitutes\n",
        "    substitutes = set(substitutes)\n",
        "    return substitutes #replace for part 1\n",
        "\n",
        "def smurf_predictor(context : Context) -> str:\n",
        "    \"\"\"\n",
        "    suggest 'smurf' as a substitute for all words.\n",
        "    \"\"\"\n",
        "    return 'smurf'\n",
        "\n",
        "def wn_frequency_predictor(context : Context) -> str:\n",
        "    # Takes a context object as input and predicts the possible synonym w highest total \n",
        "    # occurance freq according to wordnet\n",
        "\n",
        "    # Sum up the occurance counts for all senses of the word if the word and target appear together \n",
        "    # in multiple synsets\n",
        "\n",
        "    # Duplicated code from the get_candidates method for finding candidate synonyms\n",
        "    occurence_freqs = {}\n",
        "    lemmas = wn.lemmas(context.lemma, context.pos)\n",
        "    for lem in lemmas:\n",
        "      synset = lem.synset()\n",
        "      for candidate in synset.lemmas():\n",
        "        # Make sure output doesn't contain the input lemma itself\n",
        "        if candidate.name() == context.lemma:\n",
        "          continue\n",
        "        else:\n",
        "          # Remove the '_' in 'turn_around' for ex and add to list\n",
        "          candidate = candidate.name().replace('_', ' ')\n",
        "          # freqs of lemma according to wordnet\n",
        "          if occurence_freqs[candidate] not in occurence_freqs:\n",
        "            occurence_freqs[candidate] = candidate.count()\n",
        "          else:\n",
        "            occurence_freqs[candidate] += candidate.count()\n",
        "\n",
        "    # highest total occurence freq according to wordnet\n",
        "    # max(iterable, key=dict.get)\n",
        "    pred = max(occurence_freqs, key = occurence_freqs.get)\n",
        "\n",
        "    # Should give ~10% precision and recall\n",
        "    return pred # replace for part 2\n",
        "\n",
        "def wn_simple_lesk_predictor(context : Context) -> str:\n",
        "    # Select a synset for the target word using WSD\n",
        "    # To perform WSD, implement the Lesk alg\n",
        "   \n",
        "   # You may want to remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    overlap = {}\n",
        "    \n",
        "    # Look at all possible synsets that the target word appears in\n",
        "    for synset in wn.lemmas(context.lemma, context.pos):\n",
        "      lemma = synset.synset()\n",
        "      def_ = lemma.definition()\n",
        "      #print(def_)\n",
        "\n",
        "      if len(wn.lemmas(context.lemma,context.pos)) != 1:\n",
        "        # Add to the def: all exs for the synset + the def and all exs for all hypernyms of the snyset\n",
        "        # all exs for synset:\n",
        "        for elt in lemma.examples():\n",
        "          def_= def_+\" \"+elt\n",
        "        # all exs of hypernyms\n",
        "        for elt in lemma.hypernyms():\n",
        "          for elem in elt.examples():\n",
        "            def_=def_+\" \"+elt.definition()+\" \"+elem\n",
        "        \n",
        "        # remove stopwords\n",
        "        # As a subtask, tokenize and normalize the defs and exs in wordnet by either\n",
        "        # looking up various tokenization methods in NLTK or using the tokenize method provided w the code\n",
        "        \n",
        "        #def_.split()\n",
        "        #print(def_)\n",
        "\n",
        "        def_=tokenize(def_)\n",
        "        cleaned_def=[]\n",
        "        for elt in def_:\n",
        "          if elt in stop_words:\n",
        "            continue\n",
        "          else:\n",
        "            cleaned_def.append(elt)\n",
        "        \n",
        "        target_sen = context.left_context + context.right_context\n",
        "        #print(target_sen)\n",
        "\n",
        "        #target_sen = tokenize(target_sen)\n",
        "        cleaned_sen = []\n",
        "        for elt in target_sen:\n",
        "          if elt in stop_words:\n",
        "            continue\n",
        "          else:\n",
        "            cleaned_sen.append(elt)\n",
        "        \n",
        "        #print(cleaned_def)\n",
        "        #print(cleaned_sen)\n",
        "        # Compute the overlap between the def of the synset and the context of the target word\n",
        "        cleaned_def = set(cleaned_def)\n",
        "        cleaned_sen = set(cleaned_sen)\n",
        "        overlap[synset] = (len(cleaned_def.intersection(cleaned_sen)))\n",
        "        #print(overlap)\n",
        "       \n",
        "    # sort dict by values from largest to smallest\n",
        "    sorted_overlap = dict(sorted(overlap.items(), key = lambda x:x[1], reverse = True))\n",
        "    #print(sorted_overlap)\n",
        "\n",
        "    if sorted_overlap:\n",
        "      max_overlap = max(sorted_overlap.values())\n",
        "    else:\n",
        "      max_overlap = 0\n",
        "\n",
        "    keys = []\n",
        "    for key, value in overlap.items():\n",
        "      if max_overlap== sorted_overlap:\n",
        "        # If there is not overlap (ie. tie), select the most frequent synset\n",
        "        # ie the synset with which the target word forms the most frequent lexeme\n",
        "        continue\n",
        "      else:\n",
        "        if value == max_overlap:\n",
        "          keys.append(key)\n",
        "\n",
        "    # Then, select the most frequent lexeme from that synset as the result\n",
        "    \n",
        "    #max_counts = -1\n",
        "    #count = 0\n",
        "    counts = {}\n",
        "    for synset in wn.lemmas(context.lemma, context.pos): \n",
        "      if synset in keys:\n",
        "        for elt in synset.synset().lemmas():\n",
        "          #count+=elt.count()\n",
        "          counts[elt.name()] = elt.count()\n",
        "          #if max_count<count:\n",
        "            #most_freq_syn = synset\n",
        "            #max_count = count\n",
        "\n",
        "    # Return the most frequent synonym from that synset as a substitute\n",
        "    counts = dict(sorted(counts.items(),key=lambda x:x[1], reverse=True))\n",
        "    for syn in list(counts.keys()):\n",
        "      if syn != context.lemma:\n",
        "        return syn\n",
        "        #return elt.replace(\"_\",\"\") # replace for part 3\n",
        "\n",
        "class Word2VecSubst(object):\n",
        "        \n",
        "    def __init__(self, filename):\n",
        "        self.model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)    \n",
        "\n",
        "    def predict_nearest(self,context : Context) -> str:\n",
        "        # First obtain a set of possible synonyms from wordnet either by using the method from part 1 or rewriting the code\n",
        "        possible_syns = get_candidates(context.lemma, context.pos)\n",
        "        \n",
        "        max_similarity = self.model.similarity(possible_syns[0],context.lemma)\n",
        "\n",
        "        # Return the synonym that is most similar to the target word, according to the word2vec embeddings\n",
        "        for i in range(1,len(possible_syns)):\n",
        "          old_max_similarity = max_similarity\n",
        "          max_similarity = max(old_max_similarity, self.model.similarity(possible_syns[i],context.lemma))\n",
        "          if max_similarity == old_max_similarity:\n",
        "            synonym = possible_syns[i]\n",
        "          else:\n",
        "            synonym = possible_syns[i-1]\n",
        "\n",
        "        # Precision and recall should be ~11%\n",
        "        return synonym #replace for part 4\n",
        "  \n",
        "    def best_predictor(self, context : Context) -> str: \n",
        "      #PART 6 HERE, can try to calculate a weighted similarity sum \n",
        "      d = {}\n",
        "      #stop_words = stopwords.words('english') \n",
        "\n",
        "      # obtain a set of candidate syns using get_candidates\n",
        "      candidate_syns = get_candidates(context.lemma, context.pos)\n",
        "          \n",
        "      w1 = 0.5\n",
        "      w2 = 0.5\n",
        "      w3 = 0.5\n",
        "      for syn in candidate_syns:\n",
        "        try:\n",
        "          output1 = self.model.similarity(syn, context.lemma)\n",
        "          output2 = self.model.similarity(syn, context.left_context[-1])\n",
        "          output3 = self.model.similarity(syn, context.right_context[0])\n",
        "        except KeyError:\n",
        "          continue\n",
        "          \n",
        "        if syn in d:\n",
        "          d[syn] += w1*output1\n",
        "          d[syn] += w2*output2\n",
        "          d[syn] += w3*output3\n",
        "        else:\n",
        "          d[syn] = w1*output1\n",
        "          d[syn] += w2*output2\n",
        "          d[syn] += w3*output3\n",
        "        \n",
        "      high_low_d = dict(sorted(d.items(), key=lambda x:x[1], reverse=True))\n",
        "\n",
        "      return list(high_low_d.keys())#[0]\n",
        "\n",
        "\n",
        "class BertPredictor(object):\n",
        "\n",
        "    def __init__(self): \n",
        "        self.tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.model = transformers.TFDistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def predict(self, context : Context) -> str:\n",
        "        # feed the context sentence into BERT and it will tell us good replacements for the target word\n",
        "        # output vector is length |V| for each word, coontains score for each\n",
        "\n",
        "        # obtain a set of candidate syns using get_candidates\n",
        "        candidate_syns = get_candidates(context.lemma, context.pos)\n",
        "        \n",
        "        # Convert the info in context into a suitable masked input repr for the DisttilBERT model\n",
        "        # DistilBERT: feed the context sentence into BERT and it will tell us good replacements for the target word\n",
        "        # need: context sentence\n",
        "        context_sentence = \"\"\n",
        "        for left in context.left_context:\n",
        "          is_str = left.isalpha()\n",
        "          if not is_str:\n",
        "            context_sentence += left\n",
        "          else:\n",
        "            context_sentence = context_sentence + \" \" + left\n",
        "        context_sentence = context_sentence + \" \" + \"[MASK]\"\n",
        "\n",
        "        for right in context.right_context:\n",
        "          is_str = right.isalpha()\n",
        "          if not is_str:\n",
        "            context_sentence += right\n",
        "          else:\n",
        "            context_sentence = context_sentence + \" \" + right\n",
        "\n",
        "        # convert sentence to list\n",
        "        li = list(context_sentence.split(\" \"))\n",
        "        #print(li)\n",
        "        # store the index of the masked target word\n",
        "        masked_idx = li.index(\"[MASK]\")\n",
        "\n",
        "        #print(masked_idx)\n",
        "\n",
        "        # Run the DistilBERT model on the input repr\n",
        "        # output vector is length |V| for each word, coontains score for each\n",
        "        output = self.model.predict(np.array(li).reshape((1, -1))\n",
        "     \n",
        "        # Select from the set of wordnet derived candidate syns the highest-scoring word in target position\n",
        "        # ie. the position of the masked word\n",
        "        word = output[masked_idx]\n",
        "        return word\n",
        "\n",
        "    \n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    # At submission time, this program should run your best predictor (part 6).\n",
        "\n",
        "    #W2VMODEL_FILENAME = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "    #predictor = Word2VecSubst(W2VMODEL_FILENAME)\n",
        "    #predictor = BertPredictor()\n",
        "\n",
        "    for context in read_lexsub_xml(\"lexsub_trial.xml\"):#sys.argv[1]):\n",
        "        #print(context)  # useful for debugging\n",
        "        #prediction = smurf_predictor(context) \n",
        "        prediction = predictor.best_predictor(context)\n",
        "        #prediction = wn_simple_lesk_predictor(context)\n",
        "        #prediction = predictor.predict(context)\n",
        "        print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-5f922b8710b9>\"\u001b[0;36m, line \u001b[0;32m296\u001b[0m\n\u001b[0;31m    return output[masked_idx]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}